{"meta":{"title":"2YO's BLOG","subtitle":"因为热爱，所以坚持","description":"","author":"2YO","url":"https://2yo.cc","root":"/"},"pages":[{"title":"About","date":"2019-11-29T05:30:04.000Z","updated":"2020-02-04T12:16:41.260Z","comments":true,"path":"about/index.html","permalink":"https://2yo.cc/about/index.html","excerpt":"","text":"一只持续自闭中的少年 性别男，爱好美烧酒 不挑食，是个好宝宝 富强 民主 文明 和谐 自由 平等 公正 法制 爱国 敬业 诚信 友善 喜欢的Vtb: Akai Haato(单推Men 喜欢的音乐类型: 电音 or Light Music"},{"title":"Friends","date":"2019-12-17T04:05:00.000Z","updated":"2020-01-20T02:53:22.543Z","comments":true,"path":"friends/index.html","permalink":"https://2yo.cc/friends/index.html","excerpt":"","text":"这是一些大佬们 Pelom —— 夢に僕らで帆を張って来るべき日のために夜を超え"},{"title":"Music","date":"2019-11-29T05:45:58.000Z","updated":"2020-03-09T04:01:28.603Z","comments":true,"path":"music/index.html","permalink":"https://2yo.cc/music/index.html","excerpt":"","text":"这里是我最近常听的几首歌，部分可能由于版权暂时无法播放哦~如果页面有异常，请刷新重试！"}],"posts":[{"title":"关于前端反调试","slug":"关于前端反调试","date":"2020-02-28T10:48:19.000Z","updated":"2020-02-28T11:42:22.565Z","comments":true,"path":"2020/02/28/关于前端反调试/","link":"","permalink":"https://2yo.cc/2020/02/28/%E5%85%B3%E4%BA%8E%E5%89%8D%E7%AB%AF%E5%8F%8D%E8%B0%83%E8%AF%95/","excerpt":"","text":"前言写爬虫最重要的一步就是分析前后端交互的逻辑，免不了使用DevTools进行调试，一些不想别人看见自己代码的人就在这里做起了文章，不过根据我的经验越是盗版起家的网站越是反调试，反倒是大公司连加密都不加密随便你看。这篇文章会不断的更新一些反调试的分析过程，当然我并非是抱有恶意去分析这些代码的，单单是为了研究和学习。 第一节setInterval(function() { check(); }, 1000); var check = function() { let tmp = 0; function doCheck(a) { if ((\"\" + a / a)[\"length\"] !== 1 || a % 20 === 0) { (function() {} [\"constructor\"](\"debugger\")()) } else { (function() {} [\"constructor\"](\"debugger\")()) } tmp++; doCheck(++a); } try { doCheck(0) } catch(err) { //console.log(err); } finally { console.log(tmp); } }; check(); 这段代码本质上就是打开控制台后通过不断的打断点来干扰调试，不过不能通过Deactivate breakpoints跳过所有断点，这是个递归函数很快就会耗光内存和CPU。这段代码有个很大的bug，又或者是作者留的“后门”，就是把check函数暴露了出来。打开控制台后输入check = function(){};，关闭控制台再打开，解决问题。 第二节let el = new Image(); Object.defineProperty(el,'id',{get: function(){ window.location.href=\"%2F\" }}); console.log(el); 这段代码会在打开控制台后跳到主页，是直接嵌在html文档中的，由于Chrome拓展和油猴脚本(Tampermonkey)的局限性脚本注入没能尝试成功。其实有个更简单的方法，Object.defineProperty()是ES6标准，直接换个不支持ES6的浏览器就OK了。 第三节这个有些麻烦，同样是打开Chrome控制台后跳到主页，view source之后发现js代码加了密，不可能一个一个去分析，遂即尝试搜索关键字，跳转的语句一般用window.location.href=，那么就搜索window、location和href三个关键字，当搜索href时发现了这段代码： ('(0(){1.addListener(0(2,detail){if(2){3 4=self;3 5=4.location;5.href=decodeURIComponent(\\'%2f\\')}else{}});1.lanuch()})();',[],6,'function|devtoolsDetector|isOpen|const|_s|_l'.split('|'),0,{}) 把整个js文件提取出来测试一下，正是这个文件，注释掉这段代码后不再跳回主页。那么首先尝试的是直接禁止浏览器请求这个js文件，发现网页运行不正常，说明这个js文件里有核心代码或者被插了眼。之后采用Nginx反代并替换这段代码解决了问题，当然也可以使用Fiddler等工具，使用Nginx还要改一下Host，防止前端检查Referer。 其实我一开始不是搜索字符串的，因为反调试一般都是用的奇淫巧技，并不一定所有的浏览器都支持，换了个浏览器之后打开控制台直接蹦出来一个断点，估计是作者为了适不同浏览器的备选方案，也正是因为这个断点省了不少力气，通过这个断点单步调试直接定位到关键代码。 server { listen 80; server_name localhost; location ~ / { proxy_pass &#39;https://ip_addr&#39;; proxy_set_header Host &#39;domain&#39;; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_set_header referer &#39;https://domain/&#39; proxy_redirect off; sub_filter &quot;key code&quot; &quot;new code&quot;; # 替换 sub_filter_once off; # 替换所有 sub_filter_types *; # 过滤所有类型 } }","categories":[],"tags":[{"name":"Web","slug":"Web","permalink":"https://2yo.cc/tags/Web/"}]},{"title":"利用特征点检测与匹配实现维基萌制卡判重","slug":"利用特征点检测与匹配实现维基萌制卡判重","date":"2020-02-26T02:17:38.000Z","updated":"2020-02-26T03:11:14.292Z","comments":true,"path":"2020/02/26/利用特征点检测与匹配实现维基萌制卡判重/","link":"","permalink":"https://2yo.cc/2020/02/26/%E5%88%A9%E7%94%A8%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%8C%B9%E9%85%8D%E5%AE%9E%E7%8E%B0%E7%BB%B4%E5%9F%BA%E8%90%8C%E5%88%B6%E5%8D%A1%E5%88%A4%E9%87%8D/","excerpt":"","text":"起因随着广树大佬的维基萌抽卡系统卡牌数量的不断增加，给制卡过程中判断卡牌是否重复带来了困难，于是想写一个程序辅助判断卡牌是否重复。本质上的任务就是判断两张图像是否是同一张，要能适应图像的缩放、裁剪和旋转。使用哈希(aHash、pHash和dHash)办法？经过测试，哈希方法并不能很好的适应图像的缩放、裁剪和旋转，模板匹配又不能很好地适应缩放和旋转，遂即采用特征点检测和匹配算法，当然也可以试试已经LIRE等CBIR引擎。关于特征点检测和匹配会单独开篇文章仔细研究，本篇文章仅记录卡牌判重程序的编写过程。 思路可以将已经制好的卡牌全部拉下来，特征点检测后将描述子保存起来，每次制卡前将原始图像与所有卡牌进行匹配，即可判断是否重复。 开始预处理与特征检测为了避免卡牌边框对检测带来的影响，首先需要裁减掉卡牌的边框，裁剪之后进行特征检测。 orb = cv2.ORB_create() # ORB算法 card = cv2.imread(card_path)[80: 470, 85:370] card_gray = cv2.cvtColor(card, cv2.COLOR_BGR2GRAY) kp1, des1 = orb.detectAndCompute(card_gray, None) # des是描述子 由于原始图像比较大，需要原始图像缩放，之后进行特征提取。 picture = cv2.imread(picture_path) picture = cv2.resize(picture, (picture.shape[1] // 2, picture.shape[0] // 2)) picture_gray = cv2.cvtColor(picture, cv2.COLOR_BGR2GRAY) kp2, des2 = orb.detectAndCompute(picture_gray, None) 绘制特征点image3 = cv2.drawKeypoints(card, kp1, card, color=(255, 0, 255)) image4 = cv2.drawKeypoints(picture, kp2, picture, color=(255, 0, 255)) 特征匹配并筛选优秀点# BFMatcher解决匹配 bf = cv2.BFMatcher() matches = bf.knnMatch(des1, des2, k=2) # Lowe's algorithm 获取优秀匹配点 good = [] for m, n in matches: if m.distance &lt; 0.7 * n.distance: good.append([m]) print('优秀匹配点数目: %d' % len(good)) 关于Ratio值，Lowe推荐ratio的阈值为0.8，但作者对大量任意存在尺度、旋转和亮度变化的两幅图片进行匹配，结果表明ratio取值在0. 4~0. 6 之间最佳，小于0. 4的很少有匹配点，大于0. 6的则存在大量错误匹配点，所以建议ratio的取值原则如下:ratio=0. 4：对于准确度要求高的匹配； ratio=0. 6：对于匹配点数目要求比较多的匹配； ratio=0. 5：一般情况下。不过经过我的测试，0.7的情况下，当优秀点超过十个时，判断为重复图像的效果比较好。 image = cv2.drawMatchesKnn(card, kp1, picture, kp2, good, None, flags=2) 优秀点的数量为:50。多几幅图试试。 优秀点数量为:24。 优秀点数量为:18。 优秀点数量为:37。 最终效果 参考文章OpenCV探索之路（二十三）：特征检测和特征匹配方法汇总","categories":[],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://2yo.cc/tags/OpenCV/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://2yo.cc/tags/Computer-Vision/"},{"name":"ORB","slug":"ORB","permalink":"https://2yo.cc/tags/ORB/"}]},{"title":"用图床看视频是什么体验之原理分析","slug":"用图床看视频是什么体验之原理分析","date":"2020-02-20T08:11:41.910Z","updated":"2020-02-20T08:42:38.486Z","comments":true,"path":"2020/02/20/用图床看视频是什么体验之原理分析/","link":"","permalink":"https://2yo.cc/2020/02/20/%E7%94%A8%E5%9B%BE%E5%BA%8A%E7%9C%8B%E8%A7%86%E9%A2%91%E6%98%AF%E4%BB%80%E4%B9%88%E4%BD%93%E9%AA%8C%E4%B9%8B%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","excerpt":"","text":"起因前几天群友在群里分享了一篇文章用图床看视频是什么体验，今天腾出来时间研究原理并实现一下。 分析按照作者博客所说的，是直接将视频分段之后拼接在图片之后，播放时跳过前面图片部分即可。为什么不直接修改文件拓展名呢？作者提到图床站会检查文件头，如果不是图片的文件头人家是不收的，因此要在前面拼接一张小图来应付检查。从作者的博客中拿出关键代码如下： &lt;video id=\"video\" data-url=\"https://img11.360buyimg.com/img/jfs/t1/89319/11/12602/4783/5e4947e9Ee1c0bc66/f212833e4c8c8194.jpg\" src=\"blob:https://i-meto.com/2edacb21-e054-41da-b699-e549416f93d3\">&lt;/video> &lt;script src=\"https://cdn.jsdelivr.net/npm/plyr@latest/dist/plyr.min.js\">&lt;/script> &lt;script src=\"https://cdn.jsdelivr.net/npm/hls.js@latest\">&lt;/script> &lt;script src=\"https://cdn.jsdelivr.net/npm/hlsjs-upimg-wrapper@latest\">&lt;/script> &lt;script> if (Hls.isSupported()) { const video = document.getElementById(\"video\") const player = new Plyr(video) const wrapper = new HlsjsUpimgWrapper(Hls) const hls = wrapper.createPlayer({}, { offset: 107 }) hls.loadSource(video.dataset.url) hls.attachMedia(video) } &lt;/script> 从代码中可以看出偏移量为107，即图片部分为107字节，采用HLS技术，那么第一个请求的就应该是M3U8文件，里面包含了后续各段视频的地址，把video标签中data-url指向的图片拿到看看。 JPEG图像有一个很明显的特征，就是以FF D8开始，以FF D9结束，那么FF D9之后的字节不出意外就是M3U8文件。这里可以看出M3U8文件用过BASE64编码了，把它拿出来解码看看。 #EXTM3U #EXT-X-VERSION:3 #EXT-X-MEDIA-SEQUENCE:0 #EXT-X-ALLOW-CACHE:YES #EXT-X-TARGETDURATION:11 #EXTINF:10.719044, https://img11.360buyimg.com/img/jfs/t1/108612/15/6116/4263383/5e4947d2Ea258c94c/39f32d86dea1b392.jpg ......省略若干行...... #EXTINF:5.338667, https://ask.qcloudimg.com/draft/1134330/q492o838cn.jpg #EXTINF:5.130133, https://ask.qcloudimg.com/draft/1134330/u0wg2wg9j4.jpg #EXTINF:10.051711, https://ask.qcloudimg.com/draft/1134330/r0ds1qugew.jpg #EXT-X-ENDLIST这就是解码之后的M3U8文件，里面指向了各分段视频的地址（实际上是图片+视频的混合），随机从里面下载一张“图片”，去掉头，把ts文件提取出来。 with open('../data/dc30b8f2388a876a.jpg', 'rb') as f: data = f.read() jpg, ts = data[:107], data[107:] \"\"\" jpg 值为 b'\\xff\\xd8\\xff\\xdb\\x00C\\x00\\x03\\x02\\x02\\x02\\x02\\x02\\x03\\x02\\x02\\x02\\x03\\x03\\x03\\x03\\x04\\x06\\x04\\x04\\x04\\x04\\x04\\x08\\x06\\x06\\x05\\x06\\t\\x08\\n\\n\\t\\x08\\t\\t\\n\\x0c\\x0f\\x0c\\n\\x0b\\x0e\\x0b\\t\\t\\r\\x11\\r\\x0e\\x0f\\x10\\x10\\x11\\x10\\n\\x0c\\x12\\x13\\x12\\x10\\x13\\x0f\\x10\\x10\\x10\\xff\\xc9\\x00\\x0b\\x08\\x00\\x01\\x00\\x01\\x01\\x01\\x11\\x00\\xff\\xcc\\x00\\x06\\x00\\x10\\x10\\x05\\xff\\xda\\x00\\x08\\x01\\x01\\x00\\x00?\\x00\\xd2\\xcf \\xff\\xd9' \"\"\" with open('dc30b8f2388a876a.ts', 'wb') as f: f.write(ts) 经过测试ts文件是可以正常播放的，至此分析过程结束。 实现过程首先用FFmpeg把MP4转换成HLS，命令为：ffmpeg -i source.mp4 -codec:v libx264 -codec:a mp3 -map 0 -f ssegment -segment_format mpegts -segment_list playlist.m3u8 -segment_time 10 out%03d.ts 之后将上面jpg的值和各ts段的值拼接。 def process_segment(self): head = read_bytes('./head.bin') # head即为上面的jpg值 self.done_segments = [] for segment in self.segments: # segment即为各ts的byte数组 self.done_segments.append(head + segment) # 拼接 之后完成M3U8的构造。 def process_net_m3u8(m3u8, net_path): lines = m3u8.splitlines() num = len(lines) cur, cnt = 0, 0 while cur &lt; num: if '#EXTINF' in lines[cur]: cur += 1 lines[cur] = net_path[cnt] cnt += 1 else: cur += 1 m3u8_file = '\\r\\n'.join(lines) head = read_bytes('./head.bin') done = base64.b64encode(m3u8_file.encode('utf-8')) return head + done 主要的程序已经编写完毕，一开始将图片上传到SM.MS，发现下载实在是太慢了，5M要下载1min还多，而5M的视频才8秒左右，这是无法容忍的，干脆弄到GitHub上展示一下效果算了。 if (Hls.isSupported()) { const video = document.getElementById(\"video\") const player = new Plyr(video) const wrapper = new HlsjsUpimgWrapper(Hls) const hls = wrapper.createPlayer({}, { offset: 107 }) hls.loadSource(video.dataset.url) hls.attachMedia(video) } 结束既然原作者不愿意开源，要尊重原作者的想法和创意，那我也不贴代码了，哈哈哈哈哈。 顺便我要是有Choco那样的老师，我tm上课上到爆！！！","categories":[],"tags":[]},{"title":"利用CNN完成二次元图像和三次元图像的分类","slug":"利用CNN完成二次元图像和三次元图像的分类","date":"2020-02-19T10:14:47.000Z","updated":"2020-02-19T12:03:22.639Z","comments":true,"path":"2020/02/19/利用CNN完成二次元图像和三次元图像的分类/","link":"","permalink":"https://2yo.cc/2020/02/19/%E5%88%A9%E7%94%A8CNN%E5%AE%8C%E6%88%90%E4%BA%8C%E6%AC%A1%E5%85%83%E5%9B%BE%E5%83%8F%E5%92%8C%E4%B8%89%E6%AC%A1%E5%85%83%E5%9B%BE%E5%83%8F%E7%9A%84%E5%88%86%E7%B1%BB/","excerpt":"","text":"起因刚刚在Danbooru上抓了近百万张图，结果里面竟然混有不少的三次元图，如果人工一张一张的挑选就太麻烦了，就想写程序来解决。一开始想的是用Sequential跑个三层模型出来，又想到是不是有更简单的方法，就去网上查了查，但是没有找到相关的文章。之后去群里问了问，纸片协会的一喵大佬给出了两个方法，又甩出一个12页的全英文论文，无奈能力不足，搞了许久也没能搞懂，只能绕回到神经网络了。 先是写了三层神经网络模型，发现效果不是特别的理想，又重新写了卷积神经网络，这就避免了将图像展开丢失了很多重要的信息这一弊端。 准备数据集既然是二次元图像和三次元图像的分类，自然要准备二次元和三次元两类图像，各准备了1600张左右，这里有一个注意点，数据集尽量要覆盖全面，例如三次元数据集要包括人物、动物以及自然景物等，越全面分类越精确，将数据集放到acgn和real文件夹中。 编写程序导入必要的包文件from model_name.simple_vggnet import SimpleVGGNet from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.optimizers import SGD from keras.preprocessing.image import ImageDataGenerator from my_utils import utils_paths import matplotlib.pyplot as plt import numpy as np import argparse import random import pickle import cv2 import os 编写参数解析其中dataset指明数据集的路径，model指明模型输出的文件名，label_bin指明标签文件名，plot为数据可视化的图片名。 # 输入参数 ap = argparse.ArgumentParser() ap.add_argument(\"-d\", \"--dataset\", required=True, help=\"path to input dataset of images\") ap.add_argument(\"-m\", \"--model\", required=True, help=\"path to output trained model\") ap.add_argument(\"-l\", \"--label-bin\", required=True, help=\"path to output label binarizer\") ap.add_argument(\"-p\", \"--plot\", required=True, help=\"path to output accuracy/loss plot\") args = vars(ap.parse_args()) 读取图像数据# 读取数据和标签 print(\"[INFO] loading images...\") data = [] labels = [] # 拿到路径 imagePaths = sorted(list(utils_paths.list_images(args[\"dataset\"]))) random.seed(42) random.shuffle(imagePaths) # 读取数据 for imagePath in imagePaths: image = cv2.imread(imagePath) try: image = cv2.resize(image, (64, 64)) data.append(image) label = imagePath.split(os.path.sep)[-2] labels.append(label) except: pass 这里有三个注意点，分别是： 要打乱数据集中图像的顺序，且训练过程中应保持数据集中图像顺序不变，这样可以避免偶然性和利于对比，random.seed()和random.shuffle()完成了这项工作。 数据集过大，无法保证每张图都是有效的，因此数据集比1600 + 1600大出很多，在resize时捕获了异常，无效图片直接丢掉。 label = imagePath.split(os.path.sep)[-2]这是很巧妙的一行代码，因为我们将数据集分类丢到acgn和real文件夹，那么图像的路径为xxx/acgn/xxx.jpg或者 xxx/real/xxx.jpg，将路径按os.path.sep分割开来，自然倒数第二个元素就是图像的标签。 图像归一化处理# 预处理 data = np.array(data, dtype=\"float\") / 255.0 labels = np.array(labels) 数据集切分将数据集分割成训练集和测试集 # 数据集切分 (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42) 标签转换将标签转换为one-hot形式 # 标签转换 lb = LabelBinarizer() trainY = lb.fit_transform(trainY) testY = lb.transform(testY) tY = testY.tolist() for i in range(0, len(tY)): if tY[i][0] == 1: tY[i].insert(0, 0) else: tY[i].insert(0, 1) testY = np.array(tY) aY = trainY.tolist() for i in range(0, len(aY)): if aY[i][0] == 1: aY[i].insert(0, 0) else: aY[i].insert(0, 1) trainY = np.array(aY) 图像增强# 数据增强 aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\") 建立卷积神经网络# 建立卷积神经网络 model = SimpleVGGNet.build(width=64, height=64, depth=3, classes=len(lb.classes_)) 初始化参数# 初始化参数 INIT_LR = 0.01 EPOCHS = 50 BS = 32 训练神经网络# 损失函数 print(\"[INFO] 训练网络...\") opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS) model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]) # 训练网络 H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS) 测试模型# 测试 print(\"[INFO] 测试网络...\") predictions = model.predict(testX, batch_size=32) print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_)) 数据可视化# 展示结果 N = np.arange(0, EPOCHS) plt.style.use(\"ggplot\") plt.figure() plt.plot(N, H.history[\"loss\"], label=\"train_loss\") plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\") plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\") plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\") plt.title(\"Training Loss and Accuracy (SmallVGGNet)\") plt.xlabel(\"Epoch #\") plt.ylabel(\"Loss/Accuracy\") plt.legend() plt.savefig(args[\"plot\"]) 保存图像和标签# 保存模型 print(\"[INFO] 保存模型...\") model.save(args[\"model\"]) f = open(args[\"label_bin\"], \"wb\") f.write(pickle.dumps(lb)) f.close() 测试模型训练完之后当然要测试了，找几张没有出现在数据集中的图片用训练好的模型测试一下，注意，一定要用没有出现在数据集中的图片。 二次元图像测试 貌似效果还不错？甚至出现了可信度100%的情况，分对了5张，奇怪的是第三张被分为三次元且可信度为92.37%？让我们仔细观察一下为什么会出现这种情况。我们可以明显看出第三幅图与其他五张的风格明显不同，说是风格不同，不如说是作画工具不同，第三幅图可能是用水彩或者彩铅绘制的，查一下来源果然如此，是一副水彩画。 再找一幅水彩画试试。 同样被识别为三次元图像，但是可信度仅为64.93%，要解决这个问题其实也简单，准备一些水彩画丢进数据集就可以了，这里就是因为缺少水彩画的数据才导致了误判。 三次元图像测试 效果还不错？ 补充代码测试用代码# 导入所需工具包 from keras.models import load_model import argparse import pickle import cv2 # 设置输入参数 ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required=True, help=\"path to input image we are going to classify\") ap.add_argument(\"-m\", \"--model\", required=True, help=\"path to trained Keras model\") ap.add_argument(\"-l\", \"--label-bin\", required=True, help=\"path to label binarizer\") ap.add_argument(\"-w\", \"--width\", type=int, default=64, help=\"target spatial dimension width\") ap.add_argument(\"-e\", \"--height\", type=int, default=64, help=\"target spatial dimension height\") ap.add_argument(\"-f\", \"--flatten\", type=int, default=0, help=\"whether or not we should flatten the image\") args = vars(ap.parse_args()) # 加载测试数据并进行相同预处理操作 image = cv2.imread(args[\"image\"]) output = image.copy() image = cv2.resize(image, (args[\"width\"], args[\"height\"])) # scale the pixel values to [0, 1] image = image.astype(\"float\") / 255.0 # 是否要对图像就行拉平操作 if args[\"flatten\"] > 0: image = image.flatten() image = image.reshape((1, image.shape[0])) # CNN的时候需要原始图像 else: image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) # 读取模型和标签 print(\"[INFO] loading network and label binarizer...\") model = load_model(args[\"model\"]) lb = pickle.loads(open(args[\"label_bin\"], \"rb\").read()) # 预测 preds = model.predict(image) print(preds) # 得到预测结果以及其对应的标签 i = preds.argmax(axis=1)[0] label = lb.classes_[i] # 在图像中把结果画出来 text = \"{}: {:.2f}%\".format(label, preds[0][i] * 100) cv2.putText(output, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2) # 绘图 cv2.imshow(\"Image\", output) cv2.waitKey(0) utils_paths.pyimport os image_types = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\") def list_images(basePath, contains=None): # return the set of files that are valid return list_files(basePath, validExts=image_types, contains=contains) def list_files(basePath, validExts=None, contains=None): # loop over the directory structure for (rootDir, dirNames, filenames) in os.walk(basePath): # loop over the filenames in the current directory for filename in filenames: # if the contains string is not none and the filename does not contain # the supplied string, then ignore the file if contains is not None and filename.find(contains) == -1: continue # determine the file extension of the current file ext = filename[filename.rfind(\".\"):].lower() # check to see if the file is an image and should be processed if validExts is None or ext.endswith(validExts): # construct the path to the image and yield it imagePath = os.path.join(rootDir, filename) yield imagePath 参考内容部分代码参考自阿里云","categories":[],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://2yo.cc/tags/Machine-Learning/"}]},{"title":"Python使用pymongo操作MongoDB","slug":"Python使用pymongo操作MongoDB","date":"2020-02-04T12:17:13.000Z","updated":"2020-02-10T09:32:03.828Z","comments":true,"path":"2020/02/04/Python使用pymongo操作MongoDB/","link":"","permalink":"https://2yo.cc/2020/02/04/Python%E4%BD%BF%E7%94%A8pymongo%E6%93%8D%E4%BD%9CMongoDB/","excerpt":"","text":"安装与更新pymongo安装pip3 install pymongo更新pip3 install --upgrade pymongo创建数据库# coding=utf-8 import pymongo mongo_client = pymongo.MongoClient(\"mongodb://&lt;host>:&lt;port>/\") db = mongo_client[\"database_name\"] 在MongoDB中，当数据库中存在内容时，数据库才会真正被创建 查看已存在的数据库db_list = mongo_client.list_database_names() for db_name in db_list: print(db_name) 创建集合# coding=utf-8 import pymongo mongo_client = pymongo.MongoClient(\"mongo://&lt;host>:&lt;port>/\") db = mongo_client[\"database_name\"] collection = db[\"collection_name\"] 在MongoDB中，只有集合中存在内容时，集合才会真正被创建 查看已存在的集合collection_list = db.list_collection_names() for collection_name in collection_list: print(collection) 插入文档插入单个文档插入单个文档使用insert_one()方法，如： doc = {\"name\": \"Jack Ma\", \"age\": 50} result = collection.insert_one(doc) print(result) 输出结果为： &lt;pymongo.results.InsertOneResult object at 0x1abcedfgh&gt;insert_one()方法返回的是InsertOneResult对象，它包含插入文档的id值： print(result.inserted_id) # 输出为 293adbeehfa0fba39fc 插入多个文档插入多个文档使用insert_many()方法，如： docs = [ {\"name\": \"Jack Ma\", \"age\": 50}, {\"name\": \"Pony Ma\", \"age\": 45}, {\"name\": \"Neo\", \"age\": 31} ] result = collection.insert_many(docs) print(result.inserted_ids) 输出结果为： [ObjectId(&#39;5b236aa9c315aaaaaaa&#39;), ObjectId(&#39;5b236aa9c3153bbbbbbbbbb&#39;), ObjectId(&#39;5b236aa9c315ccccccccc&#39;), ObjectId(&#39;5b236aa9c31dddddddddddd&#39;), ObjectId(&#39;5b236aa9c3eeeeeeeeeeee&#39;)]insert_many方法返回InsertManyResult对象，它包含所有插入文档的id值(即insert_ids)。 指定插入文档的id值我们也可以自己指定插入文档的id值，其表现在_id属性中，id值默认是不可以重复的，如： docs = [ {\"_id\": 1, \"name\": \"Jack Ma\", \"age\": 50}, {\"_id\": 2, \"name\": \"Pony Ma\", \"age\": 45}, {\"_id\": 3, \"name\": \"Neo\", \"age\": 31} ] result = collection.insert_many(docs) print(result.inserted_ids) 输出结果为： [1, 2, 3]","categories":[],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://2yo.cc/tags/MongoDB/"}]},{"title":"Linux SSH ban掉IP","slug":"Linux-SSH-ban掉IP","date":"2020-02-03T09:03:59.000Z","updated":"2020-02-03T09:27:56.538Z","comments":true,"path":"2020/02/03/Linux-SSH-ban掉IP/","link":"","permalink":"https://2yo.cc/2020/02/03/Linux-SSH-ban%E6%8E%89IP/","excerpt":"","text":"起因刚刚在罗马尼亚置办了一台机子，今天发现上千条failed login attempts，一看这哪行，先改了口令，之后研究怎么加个黑名单，尝试到一定次数直接ban掉IP。 白名单和黑名单在/etc文件夹下有hosts.allow和hosts.deny两个文件，前者是白名单，后者是黑名单，录入格式为sshd:IP[/MASK]:allow和sshd:IP[/MASK]:deny ，如果是所有IP可以用sshd:ALL代替。 编写脚本自动ban掉IP下面编写一个脚本，超过两次登录错误自动加入黑名单。 Step 1编写脚本 vim /usr/local/bin/secure_ssh.sh #! /bin/bash cat /var/log/secure|awk '/Failed/{print $(NF-3)}'|sort|uniq -c|awk '{print $2\"=\"$1;}' > /usr/local/bin/black.txt for i in `cat /usr/local/bin/black.txt` do IP=`echo $i |awk -F= '{print $1}'` NUM=`echo $i|awk -F= '{print $2}'` if [ $NUM -gt 2 ];then grep $IP /etc/hosts.deny > /dev/null if [ $? -gt 0 ];then echo \"sshd:$IP:deny\" >> /etc/hosts.deny fi fi done Step 2创建记录登录失败次数的文件 touch /usr/local/bin/black.txt Step 3设定定时任务 crontab -e */5 * * * * sh /usr/local/bin/secure_ssh.sh查看定时任务 crontab -l查看登录日志 vi /var/log/secure Step 4查看效果 # hosts.deny sshd:222.186.175.23:deny sshd:222.186.30.12:deny sshd:222.186.15.10:deny sshd:222.186.15.166:deny sshd:222.186.30.167:deny sshd:222.186.30.218:deny sshd:222.186.31.135:deny sshd:222.186.31.83:deny sshd:222.186.180.130:deny sshd:222.186.42.7:deny sshd:222.186.180.142:deny sshd:104.244.79.181:deny sshd:222.186.30.145:deny sshd:222.186.30.187:deny sshd:222.186.30.31:deny清一色的江苏IP？？？","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://2yo.cc/tags/Linux/"}]},{"title":"使用Puppeteer获取Twitter API authorization和x-guest-token","slug":"使用Puppeteer获取Twitter-API-authorization和x-guest-token","date":"2020-02-03T05:55:30.000Z","updated":"2020-02-03T06:18:38.766Z","comments":true,"path":"2020/02/03/使用Puppeteer获取Twitter-API-authorization和x-guest-token/","link":"","permalink":"https://2yo.cc/2020/02/03/%E4%BD%BF%E7%94%A8Puppeteer%E8%8E%B7%E5%8F%96Twitter-API-authorization%E5%92%8Cx-guest-token/","excerpt":"","text":"前言最近一个项目需要抓取Twitter的一些数据，但是申请Twitter开发者API被拒绝了，只能研究研究网页端的API了，经过几次尝试，发现最少需要以下几个参数才能访问API： headers = { 'Referer': 'https://twitter.com/akaihaato', 'Origin': 'https://twitter.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36', 'x-guest-token': 'xxxxxxxxxxxxxxxxxxxxxx', 'authorization': 'Bearer AAAAAAAAAAAAAAAAAAAxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' } 这里的authorization和x-guest-token是由js生成的，获取起来很麻烦，如果去分析加密的js估计头发会掉光，所以曲线救国干脆跑个Headless Chromium，之后把两个参数拿出来就行了。 PuppeteerPuppeteer是谷歌官方出品的一个通过DevTools协议控制headless Chrome的Node库。可以通过Puppeteer的提供的api直接控制Chrome模拟大部分用户操作来进行UI Test或者作为爬虫访问页面来收集数据。 代码因为大部分的代码都是用Python写的，只用Nodejs运行Puppeteer，因此借助Express框架直接写个API供Python调用。 const express = require('express'); const puppeteer = require('puppeteer'); const app = express(); const twitter = async() => { let st = process.uptime(); const browser = await puppeteer.launch({headless: true}); // 无头模式，经过测试可以极大的节省时间 const page = await browser.newPage(); await page.setUserAgent('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36'); await page.goto('https://twitter.com/akaihaato', { timeout: 50000, }); let result = [{code: -1}, null]; // 如果请求失败，直接返回下标为0的错误码 await page.on('requestfinished', async (request) => { //请求完成事件 ，注意异步 if(request.resourceType() == \"xhr\" &amp;&amp; request.url().indexOf(\"https://api.twitter.com/2/timeline/profile/998336069992001537.json\") != -1) { result.push(request.headers()); } }); // 给足够的时间加载所有连接，视网络情况而定 await page.waitFor(20000); await browser.close(); let ed = process.uptime(); console.log('本次共耗时: ' + (ed - st).toFixed(2) + 's'); return result.pop(); // 加载过程会多次请求API，但是开始几次请求并不会携带需要的信息，我们返回最后一次请求 } app.get('/', (req, res) => { twitter().then((result) => { res.json(result); }); }); app.listen(8080);","categories":[],"tags":[{"name":"Puppeteer","slug":"Puppeteer","permalink":"https://2yo.cc/tags/Puppeteer/"}]},{"title":"青果IP代理使用","slug":"青果IP代理使用","date":"2020-02-01T09:35:38.000Z","updated":"2020-02-01T10:28:10.910Z","comments":true,"path":"2020/02/01/青果IP代理使用/","link":"","permalink":"https://2yo.cc/2020/02/01/%E9%9D%92%E6%9E%9CIP%E4%BB%A3%E7%90%86%E4%BD%BF%E7%94%A8/","excerpt":"","text":"前言当我们在爬取某些网站的数据时，过快的访问频率往往会触发反爬取机制，导致ip被禁止访问，使用IP代理是一个比较好的解决方案，所谓的IP代理其实就是提供若干个HTTP/HTTPS/SOCKS/L2TP等代理。今天刚好需要IP代理就去找了一下，发现青果提供了8小时的试用并且后续价格也比较划算，就先开始了试用。 开始试用 首先选择试用套餐，可以看到支持的协议有HTTP、SOCKS5。通道数指的是同一段时间可以使用的IP数，这并不是指只能使用一个IP，IP是可以随时换的。之后来到控制台查看IP详情。 其中AuthKey是访问代理时的用户名，AuthPwd是密码，上行带宽5MB/S，下行带宽10MB/S。存活周期指的是一个IP可以使用的最长时间。 青果云提供了相关的API用于资源申请、查询与释放，在使用代理之前必须先申请代理IP。 申请结果包括IP、PORT及过期时间等，查询和释放过程类似。 Python可通过如下代码检验代理的有效性及查看当前的IP。 # coding=utf-8 import requests proxies = { 'http': 'http://username:password@183.147.252.120:57049', 'https': 'https://username:password@183.147.252.120:57049' } res = requests.get('http://jsonip.com', proxies=proxies) ip = res.json()['ip'] print(ip) 其中的username和password就是上文提到的AuthKey和AuthPwd。 有关可用性具体没有测试是否能跑满标明的10MB/S，但是速度与不使用代理没有太大差异，且可用性应该高于98%。","categories":[],"tags":[]},{"title":"Centos7安装Python3.x与pip","slug":"Centos7安装python3-x与pip","date":"2020-02-01T09:06:50.000Z","updated":"2020-02-01T09:29:41.938Z","comments":true,"path":"2020/02/01/Centos7安装python3-x与pip/","link":"","permalink":"https://2yo.cc/2020/02/01/Centos7%E5%AE%89%E8%A3%85python3-x%E4%B8%8Epip/","excerpt":"","text":"前言Centos7自带的Python版本是2.7，由于目前还有很多库依赖于Python2.x，所以并不建议完全卸载2.x版本，因此需要与3.x版本共存，3.x需要自行下载编译安装。 安装Python3.x安装第三方库yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make yum install libffi-devel -y下载Python3.x# 建立新目录 sudo mkdir /usr/local/python3 # 这里以3.6为例，更多版本参见：https://www.python.org/ftp/python wget --no-check-certificate https://www.python.org/ftp/python/3.6.2/Python-3.6.2.tgz # 解压安装包 tar xzvf Python-3.6.2.tgz cd Python-3.6.2编译并安装sudo ./conigure --prefix=/usr/local/python3 sudo make sudo make install创建软链接sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3验证Python版本python3 -V安装pip下载并安装pipwget --no-check-certificate https://github.com/pypa/pip/archive/9.0.1.tar.gz tar -xzvf 9.0.1.tar.gz cd pip-9 .0.1 python3 setup.py install创建软链接sudo ln -s /usr/local/python3/bin/pip /usr/bin/pip3验证pip版本pip3 -V一个注意点当同时安装了Python2.x和Python3.x时，使用某些模块的命令需要指明Python版本，如使用Scrapy时：scrapy [option]需要替换为python3 -m scrapy [option]或者python2 -m scrapy [option]","categories":[],"tags":[]},{"title":"Kali踩坑记：中文乱码","slug":"Kail踩坑记：中文乱码","date":"2020-01-22T03:55:09.000Z","updated":"2020-01-22T04:29:19.479Z","comments":true,"path":"2020/01/22/Kail踩坑记：中文乱码/","link":"","permalink":"https://2yo.cc/2020/01/22/Kail%E8%B8%A9%E5%9D%91%E8%AE%B0%EF%BC%9A%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/","excerpt":"","text":"起因前几天用VirtualBox装Kali就一直装不上，今天闲下来按照某度上的各种方式试了一遍还是没解决问题，之后换了VMware就顺利装上了，不过中文都变成了小方块，按照网上的说法是下载了精简版Kali导致的中文字库缺失，随即进行了如下的尝试解决了这个问题。 解决步骤Step 1首先要确保locales已经安装，使用apt-get install locales命令；之后可以使用locale -a命令查看当前系统支持的字符集。 Step 2在命令行中输入dpkg-reconfigure locales进入图形化界面之后选中en_US.UTF-8和zh_CN.UTF-8，确定后，将en_US.UTF-8选为默认。 ps：空格选中，Tab切换。 Step 3安装中文字体，输入apt-get install xfonts-intl-chinese和apt-get install ttf-wqy-microhei命令，之后reboot重启系统。 ps：如果不出意外到此步骤已经解决问题，如果出现unable to locate package xxx的错误，需按照Step 4设置下载源。 Step 4在控制台输入vi /etc/apt/sources.list，追加如下内容： #中科大 deb http://mirrors.ustc.edu.cn/kali kali-rolling main non-free contrib deb-src http://mirrors.ustc.edu.cn/kali kali-rolling main non-free contrib #阿里云 deb http://mirrors.aliyun.com/kali kali-rolling main non-free contrib deb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib #清华大学 deb http://mirrors.tuna.tsinghua.edu.cn/kali kali-rolling main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/kali kali-rolling main contrib non-free #浙大 deb http://mirrors.zju.edu.cn/kali kali-rolling main contrib non-free deb-src http://mirrors.zju.edu.cn/kali kali-rolling main contrib non-free #东软大学 deb http://mirrors.neusoft.edu.cn/kali kali-rolling/main non-free contrib deb-src http://mirrors.neusoft.edu.cn/kali kali-rolling/main non-free contrib #官方源 deb http://http.kali.org/kali kali-rolling main non-free contrib deb-src http://http.kali.org/kali kali-rolling main non-free contrib退出保存，输入apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get dist-upgrade命令，返回步骤3执行。 ps：另可通过vi /etc/default/locale修改系统默认语言。 参考文章Kali Linux更新源 kali linux 中文出现乱码问题的解决","categories":[],"tags":[{"name":"Kali","slug":"Kali","permalink":"https://2yo.cc/tags/Kali/"}]},{"title":"Cloudflare中使用301重定向","slug":"cloudflare中使用301重定向","date":"2020-01-20T12:45:36.000Z","updated":"2020-01-20T15:18:59.547Z","comments":true,"path":"2020/01/20/cloudflare中使用301重定向/","link":"","permalink":"https://2yo.cc/2020/01/20/cloudflare%E4%B8%AD%E4%BD%BF%E7%94%A8301%E9%87%8D%E5%AE%9A%E5%90%91/","excerpt":"","text":"301与302的区别301是HTTP协议状态码之一，它表示的含义是永久性的转移（Permanently Moved），而302代表的是暂时性的转移（Temporarily Moved），当用户或搜索引擎访问服务端且返回这两个状态码时，在Header中的Location字段将包含将要跳转到的网址，区别是301表示旧地址的资源已经永久的被移除了，搜索引擎在抓取新内容时也会将旧网址替换为新网址，而302表示旧资源只是暂时性的不能访问，搜索引擎不会修改旧网址。这样对搜索引擎比较友好，且对权重的影响很小。 适用情况 更换了新的域名，将旧域名重定向到新域名 网址的规范化，例如将a.com、a.a.com等全部重定向到www.a.com 将已经过期或无效的资源或者网页访问重定向到正确的网址 将 HTTP 访问重定向到 HTTPS Cloudflare中使用301Step1首先要确保旧域名存在一条解析记录，否则会提示找不到ip地址，新建一条Page Rule，Type选Forwarding URL，状态选301-Permanent Redirect 。 Step21.统一跳转即将a.com、a.com/xxx.html及a.com/xxx/xxx等无差别的重定向到b.com，写法为：a.com/* 301 b.com 2.对应跳转即将a.com、a.com/xxx.html及a.com/xxx/xxx等严格跳转到b.com、b.com/xxx.html及b.com/xxx/xxx，写法为：a.com/* 301 a.com/$1 这里还有一个注意点是，旧域名只填写域名即可，而新域名必须指定协议（http或者https），否则会提示错误。 参考文章如何设置网站 301 重定向 | 使用 Cloudflare","categories":[],"tags":[{"name":"Cloudflare","slug":"Cloudflare","permalink":"https://2yo.cc/tags/Cloudflare/"}]},{"title":"修改Hosts文件加速Github访问","slug":"修改Hosts文件加速Github访问","date":"2020-01-20T03:05:05.000Z","updated":"2020-01-20T03:14:12.094Z","comments":true,"path":"2020/01/20/修改Hosts文件加速Github访问/","link":"","permalink":"https://2yo.cc/2020/01/20/%E4%BF%AE%E6%94%B9Hosts%E6%96%87%E4%BB%B6%E5%8A%A0%E9%80%9FGithub%E8%AE%BF%E9%97%AE/","excerpt":"","text":"前几天访问Github还好好的，昨天夜里就怎么也上不去了，早上爬起来Ping了几个ip丢到hosts就好了。 如果使用Windows系统，hosts文件一般在C:\\Windows\\System32\\drivers\\etc路径下，打开之后追加如下内容即可，这些ip的访问速度可能根据你的网络运营商决定，并且可能会发生变化。 另外 githubusercontent.com 不出意外应该早就被屏蔽。 # Github 192.30.253.112 github.com 151.101.76.133 avatars1.githubusercontent.com 151.101.76.133 avatars3.githubusercontent.com 151.101.76.133 avatars2.githubusercontent.com 151.101.76.133 avatars0.githubusercontent.com 151.101.76.133 raw.githubusercontent.com","categories":[],"tags":[{"name":"Github","slug":"Github","permalink":"https://2yo.cc/tags/Github/"},{"name":"Hosts","slug":"Hosts","permalink":"https://2yo.cc/tags/Hosts/"}]},{"title":"下载喜欢的UP主的所有视频","slug":"下载喜欢的UP主的所有视频","date":"2020-01-19T02:24:03.000Z","updated":"2020-02-10T09:37:23.657Z","comments":true,"path":"2020/01/19/下载喜欢的UP主的所有视频/","link":"","permalink":"https://2yo.cc/2020/01/19/%E4%B8%8B%E8%BD%BD%E5%96%9C%E6%AC%A2%E7%9A%84UP%E4%B8%BB%E7%9A%84%E6%89%80%E6%9C%89%E8%A7%86%E9%A2%91/","excerpt":"","text":"对于喜欢的UP主，是不是有种想要把他（她）所有的投稿下载并珍藏起来的冲动呢，不过要怎么做？用APP逐个投稿缓存在转移到PC上？或者使用诸如you-get之类的第三方工具？这些都太麻烦了，需要一个一个的点进去或者输入av号，因此就写了个下载UP主所有投稿的工具，没有太多技术含量，都是一些机械工作，下面只强调几个注意点，代码开源到Github。 新版播放器采用DASH，并将音频和视频分为两个流，猜测是压缩图像时保留原质量音频？ 切换到旧版Flash即可找到flv接口，不过貌似有加密接口可以直接下载1080p。 视频服务器貌似有限速？带宽不能跑满，遂采用多线程分段下载。 使用方法1.将bilibili.py中UID修改为UP主的UID，运行bilibili.py即可。2.ignore.json用于忽略已下载的投稿，其格式为标准的json。3.文件命名规则为：av[av号]-[分P]-[分段]。如av52236157-1-1.flv。4.分段是指将较大的视频分为若干段，这些段组合起来成为一个完整的视频，不过除番剧外貌似大多数情况只分为一段。5.下载进度长时间卡住时，停止并重新运行即可，这多出现在视频比较大的情况。 GithubbilibiliUPDownloader","categories":[],"tags":[{"name":"bilibili","slug":"bilibili","permalink":"https://2yo.cc/tags/bilibili/"},{"name":"tools","slug":"tools","permalink":"https://2yo.cc/tags/tools/"}]},{"title":"利用SCF解决跨域、防盗链及HTTPS不能降级请求HTTP资源问题","slug":"利用SCF解决跨域、防盗链及HTTPS不能降级请求HTTP资源问题","date":"2020-01-11T07:31:45.000Z","updated":"2020-01-20T12:46:18.339Z","comments":true,"path":"2020/01/11/利用SCF解决跨域、防盗链及HTTPS不能降级请求HTTP资源问题/","link":"","permalink":"https://2yo.cc/2020/01/11/%E5%88%A9%E7%94%A8SCF%E8%A7%A3%E5%86%B3%E8%B7%A8%E5%9F%9F%E3%80%81%E9%98%B2%E7%9B%97%E9%93%BE%E5%8F%8AHTTPS%E4%B8%8D%E8%83%BD%E9%99%8D%E7%BA%A7%E8%AF%B7%E6%B1%82HTTP%E8%B5%84%E6%BA%90%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题阐述在Web应用开发中，我们经常会遇到跨域，Refer防盗链以及HTTPS不能降级访问HTTP资源问题，一般的浏览器禁止了跨域请求且Ajax等方式不允许修改某些Header（如Refer和Orign），针对这三种情况我们一般只能通过代理访问来解决，即在自己的服务器上写一个代理小程序，再通过自己的域名访问这个小程序来解决。但是我们本就瘦小的带宽还要分出来一部分做代理着实让人不爽，这里分享一种方法，就是通过SCF来解决。 何为SCF？SCF（Serverless Cloud Function），即无服务器云函数，是由腾讯在无服务器架构上的一种实现（阿里和华为也有，但名称不同，如称函数工作流），无服务器（Serverless）不是表示没有服务器，而表示当在使用 Serverless 时，无需关心底层资源，也无需登录服务器和优化服务器，只需关注最核心的代码片段，即可跳过复杂的、繁琐的基本工作。核心的代码片段完全由事件或者请求触发，平台根据请求自动平行调整服务资源。Serverless 拥有近乎无限的扩容能力，空闲时，不运行任何资源。代码运行无状态，可以轻易实现快速迭代、极速部署。 简而言之，就是提供了一个粘贴代码即可运行的环境，并且没有带宽约束以及“并发约束”，实现了弹性伸缩，无需担心服务器过载。 SCF目前分为三种，普通的云函数，以及Serverless2.0中的HTTP函数和服务型函数，后二者目前在内测，很荣幸拿到了HTTP函数的内测名额，普通的云函数基于事件驱动且配置麻烦限制也多，只能返回Json，如果要返回图片等媒体资源只能Base64编码后返回，大大降低了效率。而HTTP函数则不存在这种限制，它是完全基于HTTP Request 和HTTP Response的，但这并不意味着对返回是没有限制的： 使用限制 当前 HTTP 函数目前仅支持 Nodejs 8.9，后续会支持多个 Runtime。 函数类型选定为 HTTP 后不可更改。 HTTP 函数不支持设置触发器。 HTTP 函数暂不支持版本和别名。 在 Request headers 中有以下限制： 所有 key 和 value 的大小不得超过4KB。 path（含 query、params）不得超过4KB。 body 大小不超过6MB。 在 Response headers 中有以下限制： 所有 key 和 value 的大小不得超过4KB。 body 的大小不超过6MB。对body的大小限制在6MB，但对于请求Json和图片等资源足够了，目前在内测阶段，说不准以后会有变动。 一个例子下面我们通过一个例子来讲述如何通过SCF来解决上述大坑。 我们知道著名插画站Pixiv（以下简称P站）由于某些不可名状的原因被GFW拒之墙外，但是它的图片服务器域名依旧还是可以访问的，不过对图片做了防盗链处理，必须要携带Refer才能访问，否则返回403，如下图： 那么我们首先去创建一个HTTP云函数，如下图： 点击下一步，有一些函数的基本信息： 确认无误后我们点完成，这样我们就创建好了一个Hello world项目： 函数创建好之后就会为我们分配好一个公网域名，我们访问一下试试看： 基本的流程完毕了，剩下的就是写代码了，P站的防盗链机制是检查Refer，那么只要伪造Header就可以了，我已经写好一个了，代码如下（不是专业搞前端的，JS写的很烂，大佬轻喷: const fetch = require('node-fetch'); const app = require('express')(); app.get('/', (req, res) => { pic = req.query.url if (pic === undefined || pic.length == 0) { // 必须要有图片url，否则我们返回400 res.status(400).json({ 'code': -1, 'msg': 'URL is a necessary parameter.' }); } else { fetch(pic, { headers: { 'Referer': 'https://pixiv.net', } }).then(function(response) { if (!response.ok) { res.set('Content-Type', 'application/json'); res.status(500).json({ 'code': -1, 'status': 'error' }); } else { res.set('Content-Type', 'image/jpeg'); } returnresponse }).then(response => response.buffer()).then(response => res.status(200).send(response)).then(). catch(e => console.log('Msg' + e)); } }); exports.main_handler = app //app.listen(8080) //app.listen(8080) 补充SCF已经支持其他环境 参考文档腾讯SCF官方手册","categories":[],"tags":[{"name":"SCF","slug":"SCF","permalink":"https://2yo.cc/tags/SCF/"}]},{"title":"一次自动提交脚本的经历","slug":"一次自动提交脚本的经历","date":"2020-01-11T06:42:01.000Z","updated":"2020-01-20T02:58:17.030Z","comments":true,"path":"2020/01/11/一次自动提交脚本的经历/","link":"","permalink":"https://2yo.cc/2020/01/11/%E4%B8%80%E6%AC%A1%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4%E8%84%9A%E6%9C%AC%E7%9A%84%E7%BB%8F%E5%8E%86/","excerpt":"","text":"起因事情的起因是这样的，最近需要在某系统上注册一个账号，结果怎么都收不到邮件，试了有十几次，然后我去打听了一下，所有人都是这样，少则十几次才能收到，多则几十次，便萌生了写脚本自动提交的想法。 分析逻辑第一步当然要分析逻辑，这也是最重要的一步，好在这个系统比较简单，没一会儿就分析好了。先看一下表单：两个输入框分别输入邮件和验证码，我们在看下源码：这个form没有action属性，是通过checkForm方法提交的，我们再来看下checkForm方法：还有一些代码被我删减掉了，只留下最主要的代码，首先将表单数据post到后台，如果后台的返回body为空串，则跳转到第二个网址，否则提示错误信息。至此我们已经将逻辑分析好了，我们只要模拟这段js代码的逻辑向后台提交数据。 验证码识别这个系统的验证码图形并不复杂，干扰线不严重，数字整体也没有太大扭曲，所以使用Tesseract就足够了，验证码识别的准确率不需要百分之百但也不能太低，百分之六十以上就不错了。为了提高准确率，我们先要对图片做一些简单的处理。 灰度化处理第一步是将验证码处理为灰度图片，下图为处理好的例子，左侧为原图像，右侧为灰度图像。 二值化处理第二步将处理好的灰度图像二值化，二值化的阈值选择可能比较难选择，需要多次试验选择一个较优的阈值，下面为不同阈值对应的二值化图像：阈值130是一个比较好的选择，阈值100有些数字已经看不清了，200又没能很好的去除干扰线。 Tesseract识别第三步用Tesseract识别二值化图像就可以了。 5个中对了4个，就结果来说比较满意了。另外，不知您发现没有，它这个验证码每个数字的位置是固定的。。。 补充代码被我删掉了，我怕铁憨憨把人家系统弄瘫痪。。。","categories":[],"tags":[{"name":"tools","slug":"tools","permalink":"https://2yo.cc/tags/tools/"}]},{"title":"博客迁移到hexo啦","slug":"关于博客迁移","date":"2019-12-17T03:38:20.000Z","updated":"2020-02-10T09:33:58.001Z","comments":true,"path":"2019/12/17/关于博客迁移/","link":"","permalink":"https://2yo.cc/2019/12/17/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/","excerpt":"","text":"由于.cc域名暂时不能备案又不舍得丢，只好将博客迁移到Gitpage，最近会将一些有价值的文章从原博客上迁移过来，WordPress有些过于方便导致最近的文章有些水，这次就打算好好写一些长文了。并且打算用空出的服务器开发出一些小工具来供朋友们使用，毕竟一直白嫖API也怪不好意思的，哈哈哈。","categories":[],"tags":[{"name":"Other","slug":"Other","permalink":"https://2yo.cc/tags/Other/"}]}]}